{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e5c51c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46e05bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/Users/admin/Desktop/nitin/CS274/CS274_project/AGE/'\n",
    "def load_data(dataset):\n",
    "    # load the data: x, tx, allx, graph\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    if dataset == 'wiki':\n",
    "        adj, features, label = load_wiki()\n",
    "        return adj, features, label, 0, 0, 0\n",
    "\n",
    "    for i in range(len(names)):\n",
    "        '''\n",
    "        fix Pickle incompatibility of numpy arrays between Python 2 and 3\n",
    "        https://stackoverflow.com/questions/11305790/pickle-incompatibility-of-numpy-arrays-between-python-2-and-3\n",
    "        '''\n",
    "        with open(PATH+\"data/ind.{}.{}\".format(dataset, names[i]), 'rb') as rf:\n",
    "            u = pkl._Unpickler(rf)\n",
    "            u.encoding = 'latin1'\n",
    "            cur_data = u.load()\n",
    "            objects.append(cur_data)\n",
    "        # objects.append(\n",
    "        #     pkl.load(open(\"data/ind.{}.{}\".format(dataset, names[i]), 'rb')))\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(\n",
    "        \"data/ind.{}.test.index\".format(dataset))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if dataset == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(\n",
    "            min(test_idx_reorder), max(test_idx_reorder) + 1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range - min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range - min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "    \n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "    \n",
    "    idx_test = test_idx_range.tolist()\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y) + 500)\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    return adj, features, np.argmax(labels, 1), idx_train, idx_val, idx_test\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    index = []\n",
    "    for line in open(PATH+filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def load_wiki():\n",
    "    f = open(PATH+'data/graph.txt','r')\n",
    "    adj, xind, yind = [], [], []\n",
    "    for line in f.readlines():\n",
    "        line = line.split()\n",
    "        \n",
    "        xind.append(int(line[0]))\n",
    "        yind.append(int(line[1]))\n",
    "        adj.append([int(line[0]), int(line[1])])\n",
    "    f.close()\n",
    "    ##print(len(adj))\n",
    "\n",
    "    f = open(PATH+'data/group.txt','r')\n",
    "    label = []\n",
    "    for line in f.readlines():\n",
    "        line = line.split()\n",
    "        label.append(int(line[1]))\n",
    "    f.close()\n",
    "\n",
    "    f = open(PATH+'data/tfidf.txt','r')\n",
    "    fea_idx = []\n",
    "    fea = []\n",
    "    adj = np.array(adj)\n",
    "    adj = np.vstack((adj, adj[:,[1,0]]))\n",
    "    adj = np.unique(adj, axis=0)\n",
    "    \n",
    "    labelset = np.unique(label)\n",
    "    labeldict = dict(zip(labelset, range(len(labelset))))\n",
    "    label = np.array([labeldict[x] for x in label])\n",
    "    adj = sp.csr_matrix((np.ones(len(adj)), (adj[:,0], adj[:,1])), shape=(len(label), len(label)))\n",
    "\n",
    "    for line in f.readlines():\n",
    "        line = line.split()\n",
    "        fea_idx.append([int(line[0]), int(line[1])])\n",
    "        fea.append(float(line[2]))\n",
    "    f.close()\n",
    "\n",
    "    fea_idx = np.array(fea_idx)\n",
    "    features = sp.csr_matrix((fea, (fea_idx[:,0], fea_idx[:,1])), shape=(len(label), 4973)).toarray()\n",
    "    scaler = preprocess.MinMaxScaler()\n",
    "    #features = preprocess.normalize(features, norm='l2')\n",
    "    features = scaler.fit_transform(features)\n",
    "    features = torch.FloatTensor(features)\n",
    "\n",
    "    return adj, features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8baaa64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import networkx as nx\n",
    "import sklearn.preprocessing as preprocess\n",
    "adj, features, true_labels, idx_train, idx_val, idx_test = load_data('wiki')  # Load dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a11b906",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sp.csr_matrix.toarray(adj)\n",
    "y = features.numpy()\n",
    "gt = true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "284540c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2405, 2405), (2405, 4973), (2405,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape, y.shape, gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7222cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('gt.npy', gt) \n",
    "np.save('p.npy', p) \n",
    "np.save('y.npy', y) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
